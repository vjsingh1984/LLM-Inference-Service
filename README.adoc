= LLM Inference Service
:toc: left
:toclevels: 3
:sectnums:
:icons: font
:source-highlighter: rouge
:experimental:
:imagesdir: images

== Overview

The LLM Inference Service is a cost-effective, production-ready inference server that democratizes access to large language model deployment using consumer-grade hardware. Built to address the economic and technical challenges of modern LLM infrastructure, it provides a unified solution for teams who need enterprise capabilities without datacenter GPU costs.

=== Why This Project Exists

**Economic Reality**: The current LLM infrastructure landscape forces an impossible choice - either pay $30,000+ for datacenter GPUs (H100, A100) or struggle with fragmented, limited tooling. Most teams, researchers, and startups simply cannot justify the cost of datacenter hardware for experimentation and development.

**Technical Gaps**: Existing solutions have critical limitations:

* **Ollama**: Limited enterprise features, poor GPU sharding, no comprehensive monitoring
* **vLLM**: Complex multi-GPU quantization setup, steep learning curve
* **OpenAI API**: Expensive, vendor lock-in, no control over infrastructure
* **No unified dashboard**: No single interface for monitoring, debugging, and managing multiple models

**Infrastructure Challenges**: Teams need to maximize their existing hardware investment through intelligent utilization of motherboard PCIe lanes and consumer GPUs, rather than splurging on single high-VRAM datacenter cards.

=== What This Solution Provides

**Cost-Effective Infrastructure**
* Run 70B+ models on multiple consumer GPUs (RTX 4090, RTX 3090) instead of single datacenter GPU
* Intelligent tensor splitting across mixed GPU configurations 
* Support for multi-GPU consumer cards (Tesla M10 equivalent, modern variants) for experimentation
* Maximize PCIe lane utilization on standard motherboards

**Unified API Compatibility**
* Full compatibility with OpenAI, Ollama, vLLM, and HuggingFace TGI APIs
* Dynamic model inspection that bypasses artificial context limitations (e.g., 32K models showing as 4K)
* Think tag preservation for reasoning models
* Real-time streaming with progress tracking

**Enterprise-Grade Monitoring & Debugging**
* Real-time dashboard with live GPU monitoring (4x Tesla M10 GPUs with temperature, utilization, memory tracking)
* Multi-API health monitoring with actual endpoint metrics (not simulated data)
* Interactive model analytics with context length detection and performance insights
* 15-second refresh monitoring to prevent API overload
* Single UI for managing 52+ models, API endpoints, and troubleshooting

**Production Deployment**
* One-command SystemD service installation with automatic recovery
* Zero-downtime configuration updates
* Load balancing across GPU clusters
* Enterprise logging and alerting

**Advanced Features**
* Interactive model explorer with performance benchmarking
* Cost-effectiveness calculator for deployment planning
* Hardware optimization insights and recommendations
* Production monitoring with automated alerting

=== How It Works

**Smart GPU Utilization**
The service automatically detects and optimally distributes workloads across available consumer GPUs, regardless of VRAM differences or generations. This enables teams to build powerful inference clusters using affordable consumer hardware instead of expensive datacenter solutions.

**Dynamic Model Intelligence**
By integrating directly with Ollama CLI, the service dynamically discovers true model capabilities - solving the common problem where models artificially report 4K context limits when they actually support 32K+ tokens.

**Unified Architecture**
A clean, modular codebase with format-specific adapters ensures compatibility across multiple AI providers while maintaining a single codebase and deployment model.

**Real-World Impact**: This approach enables startups and research teams to experiment with state-of-the-art models using existing hardware, while providing the monitoring and debugging capabilities needed for production deployment.

== Documentation Structure

This documentation is organized into focused sections:

* **<<architecture>>**: System architecture and component design
* **<<features>>**: Feature overview and capabilities  
* **<<api-reference>>**: Complete API documentation and examples
* **<<deployment>>**: Production deployment and configuration guide
* **<<monitoring>>**: Monitoring, operations, and troubleshooting

== Architecture

include::docs/architecture.adoc[]

== Features

include::docs/features.adoc[]

== API Reference

include::docs/api-reference.adoc[]

== Deployment

include::docs/deployment.adoc[]

== Monitoring

include::docs/monitoring.adoc[]

== Quick Start

=== Installation

[source,bash]
----
# Install user-level SystemD service
./install-user-service.sh

# Start the service (runs on port 11435)
systemctl --user start llm-inference

# Access web dashboard
open http://localhost:11435/dashboard
----

=== Test API

[source,bash]
----
# List models
curl http://localhost:11435/api/models

# Test with fast model
curl -X POST http://localhost:11435/api/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "tinyllama:1.1b-chat", "messages": [{"role": "user", "content": "Hi"}]}'
----

=== Production Status

[cols="2,2,2"]
|===
|Component |Status |Details

|**Multi-API Support**
|Production Ready
|OpenAI, Ollama, vLLM, HuggingFace TGI

|**GPU Monitoring**
|Production Ready
|4x Tesla M10 with thermal management

|**Dashboard System**
|Production Ready
|6 specialized dashboards, 15s refresh

|**Model Management**
|Production Ready
|52+ models, up to 131K context detection

|**Hardware Optimization**
|Production Ready
|100.0/100 system score, intelligent recommendations
|===

=== Dashboard Pages Overview

[cols="1,3,2"]
|===
|Dashboard Page |Description |Access Path

|**Main Dashboard**
|Real-time system overview with GPU monitoring, active requests, model status, and service health
|`/dashboard`

|**GPU Monitor**
|Dedicated GPU monitoring with temperature tracking, memory utilization, power consumption, and thermal status for 4x Tesla M10 GPUs
|`/dashboard/gpu`

|**Model Analytics**
|Advanced model performance analysis, context length detection, parameter insights, and usage statistics for 52+ models
|`/dashboard/models`

|**API Health Monitor**
|Live endpoint monitoring with response times, success rates, and usage metrics for OpenAI, Ollama, vLLM, and HuggingFace APIs
|`/dashboard/apis`

|**Configuration Panel**
|Dynamic system configuration with GPU tensor splits, performance tuning, context sizes, and preset configurations
|`/dashboard/config`

|**Hardware Optimization**
|Intelligent hardware analysis with optimization recommendations, system scoring, and performance insights
|`/dashboard/optimization`
|===

=== Dashboard Features

.LLM Inference Service - Main Dashboard
image::llm-service-main-dashboard.png[Main Dashboard,1200,800]

.GPU Monitor Dashboard
image::llm-service-main-gpudetail.png[GPU Monitor,1200,800]

.Configuration Panel
image::llm-service-main-configset.png[Configuration Panel,1200,800]

.Hardware Optimization Dashboard
image::optimization-dashboard-screenshot.png[Optimization Dashboard,1200,800]

**Key Features:**
* Real-time GPU monitoring (4x Tesla M10)
* 15-second dashboard refresh intervals
* Dynamic configuration management
* Hardware optimization scoring (100.0/100 current score)
* API metrics tracking with actual endpoint data
* 52+ model management with context detection up to 131K tokens


== License

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

=== Third-Party Licenses

This project builds upon and includes code from:

* **llama.cpp**: MIT License - High-performance LLM inference engine
* **Flask**: BSD License - Web framework
* **Ollama**: MIT License - Model format and API design inspiration

See the `NOTICE` file for complete license information and attributions.

== Support

For support and questions:

* **Issues**: Create an issue in the project repository
* **Documentation**: Check this README and inline code documentation
* **Community**: Join the discussion in project forums

== Acknowledgments

This project builds upon the excellent work of:

* **llama.cpp**: High-performance LLM inference engine
* **Ollama**: Model format and API design inspiration  
* **OpenAI**: API compatibility standards
* **Flask**: Web framework foundation